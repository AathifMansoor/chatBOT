ğŸ§  Local AI Chatbot Suite (Ollama + Streamlit)

A collection of locally hosted AI chatbot applications powered by Ollama (Gemma 3) and Streamlit.

This project contains three AI applications:

ğŸ­ Character AI (Role-Based Chatbot)

ğŸ§  Local AI Chatbot (Configurable Assistant)

ğŸš€ NeuroFlux Cognitive Engine (4-Word Jargon AI)

All apps run fully offline using gemma3:latest via Ollama.

ğŸ“¦ Project Structure
.
â”œâ”€â”€ characterBOT.py     # Role-based character chatbot
â”œâ”€â”€ chatbot.py          # Configurable local chatbot
â”œâ”€â”€ jargonwhat.py       # 4-word thinking + jargon AI
â””â”€â”€ README.md

âš™ï¸ Requirements
1ï¸âƒ£ Install Ollama

Download and install Ollama:

ğŸ‘‰ https://ollama.com

Pull the required model:

ollama pull gemma3:latest


Start Ollama server:

ollama serve


Default API runs on:

http://localhost:11434

2ï¸âƒ£ Install Python Dependencies
pip install streamlit ollama requests

ğŸ­ 1. Character AI (characterBOT.py)

A role-based conversational AI with selectable personalities.

âœ¨ Features

Multiple character personas:

ğŸ‘‘ Royal Knight

ğŸŒ¸ Anime Friend

ğŸ’¼ CEO Mentor

Conversation memory

Sidebar character selector

Chat history persistence (session-based)

Uses Ollama REST API

ğŸ§  Architecture

Frontend: Streamlit

Backend: Ollama REST API (/api/generate)

Model: gemma3:latest

Non-streaming response mode

â–¶ï¸ Run
streamlit run characterBOT.py

ğŸ§  2. Local AI Chatbot (chatbot.py)

A configurable local LLM chat interface using Ollama Python SDK.

âœ¨ Features

Model selector

Adjustable temperature (0.0 â€“ 1.5)

Chat memory via st.session_state

Clear chat option

Clean centered UI

ğŸ§  Architecture

Uses ollama.chat() (native Python SDK)

Real-time LLM interaction

Session-based memory

Configurable inference parameters

â–¶ï¸ Run
streamlit run chatbot.py

ğŸš€ 3. NeuroFlux Cognitive Engine (jargonwhat.py)

A structured-thinking AI that:

Thinks step-by-step

Shows reasoning inside <thinking> tags

Replies in exactly 4 words

Uses creative business/tech jargon

No requests, time, or re modules

âœ¨ Features

Strict output formatting

High creativity temperature

Clean futuristic UI

Controlled prompt engineering

ğŸ§  Output Format Example
<thinking>
Analyzing strategic opportunity...
Optimizing solution framing...
Compressing to four-word output...
</thinking>

Final: Scalable Quantum Growth Matrix

â–¶ï¸ Run
streamlit run jargonwhat.py

ğŸ”Œ Model Configuration

All applications use:

gemma3:latest


You may change the model inside the scripts if needed.

ğŸ—ï¸ System Architecture Overview
User â†’ Streamlit UI â†’ Ollama (Local LLM) â†’ Gemma 3 â†’ Response â†’ UI


No cloud APIs required.
Fully local.
Privacy-safe.

ğŸ”’ Offline Capability

This project:

âœ… Runs completely offline
âœ… Does not require OpenAI API
âœ… Keeps all data local
âœ… No external cloud dependency

ğŸ§ª Customization Guide
Change Model

Replace:

model="gemma3:latest"


With:

model="your-model-name"

Adjust Creativity

Modify:

options={"temperature": 0.7}


Higher â†’ More creative
Lower â†’ More deterministic

ğŸ› ï¸ Troubleshooting
Ollama not responding?

Make sure:

ollama serve


is running.

Model not found?

Run:

ollama pull gemma3:latest

Port Issue?

Default Ollama API:

http://localhost:11434


Ensure nothing else uses that port.

ğŸ“ˆ Future Improvements

Streaming responses

Token usage display

Multi-model switching

Persistent database memory

Docker containerization

Authentication layer

ğŸ“„ License

This project is open-source and intended for educational and local AI experimentation purposes.

ğŸ‘¨â€ğŸ’» Author

Developed using:

Streamlit

Ollama

Gemma 3 LLM
